{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../scraper/scraped_data/data_selenium.json\"\n",
    "# Load JSON file\n",
    "with open(file_path) as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>texts</th>\n",
       "      <th>images</th>\n",
       "      <th>pdf_links</th>\n",
       "      <th>pdf_extracted</th>\n",
       "      <th>image_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.svf.gov.lk/index.php?lang=en</td>\n",
       "      <td>Shrama Vasana Fund - Home</td>\n",
       "      <td>[Menu, About Us, Contributions, Services, Down...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/homeicon.png, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/homeicon.png':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.svf.gov.lk/index.php?option=com_co...</td>\n",
       "      <td>Shrama Vasana Fund - Overview</td>\n",
       "      <td>[Menu, About Us, Contributions, Services, Down...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/homeicon.png, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/homeicon.png':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.svf.gov.lk/index.php?option=com_co...</td>\n",
       "      <td>Shrama Vasana Fund - Contributions</td>\n",
       "      <td>[Menu, About Us, Contributions, Services, Down...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/homeicon.png, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/homeicon.png':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.svf.gov.lk/index.php?option=com_co...</td>\n",
       "      <td>Shrama Vasana Fund - Services</td>\n",
       "      <td>[Menu, About Us, Contributions, Services, Down...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/homeicon.png, h...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/homeicon.png':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.svf.gov.lk/index.php?option=com_co...</td>\n",
       "      <td>Shrama Vasana Fund - Downloads</td>\n",
       "      <td>[Menu, About Us, Contributions, Services, Down...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/homeicon.png, h...</td>\n",
       "      <td>[https://www.svf.gov.lk/images/pdfs/act_en.pdf...</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/pdfs/act_en.pd...</td>\n",
       "      <td>{'https://www.svf.gov.lk/images/homeicon.png':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0           https://www.svf.gov.lk/index.php?lang=en   \n",
       "1  https://www.svf.gov.lk/index.php?option=com_co...   \n",
       "2  https://www.svf.gov.lk/index.php?option=com_co...   \n",
       "3  https://www.svf.gov.lk/index.php?option=com_co...   \n",
       "4  https://www.svf.gov.lk/index.php?option=com_co...   \n",
       "\n",
       "                                title  \\\n",
       "0           Shrama Vasana Fund - Home   \n",
       "1       Shrama Vasana Fund - Overview   \n",
       "2  Shrama Vasana Fund - Contributions   \n",
       "3       Shrama Vasana Fund - Services   \n",
       "4      Shrama Vasana Fund - Downloads   \n",
       "\n",
       "                                               texts  \\\n",
       "0  [Menu, About Us, Contributions, Services, Down...   \n",
       "1  [Menu, About Us, Contributions, Services, Down...   \n",
       "2  [Menu, About Us, Contributions, Services, Down...   \n",
       "3  [Menu, About Us, Contributions, Services, Down...   \n",
       "4  [Menu, About Us, Contributions, Services, Down...   \n",
       "\n",
       "                                              images  \\\n",
       "0  [https://www.svf.gov.lk/images/homeicon.png, h...   \n",
       "1  [https://www.svf.gov.lk/images/homeicon.png, h...   \n",
       "2  [https://www.svf.gov.lk/images/homeicon.png, h...   \n",
       "3  [https://www.svf.gov.lk/images/homeicon.png, h...   \n",
       "4  [https://www.svf.gov.lk/images/homeicon.png, h...   \n",
       "\n",
       "                                           pdf_links  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [https://www.svf.gov.lk/images/pdfs/act_en.pdf...   \n",
       "\n",
       "                                       pdf_extracted  \\\n",
       "0                                                 {}   \n",
       "1                                                 {}   \n",
       "2                                                 {}   \n",
       "3                                                 {}   \n",
       "4  {'https://www.svf.gov.lk/images/pdfs/act_en.pd...   \n",
       "\n",
       "                                     image_extracted  \n",
       "0  {'https://www.svf.gov.lk/images/homeicon.png':...  \n",
       "1  {'https://www.svf.gov.lk/images/homeicon.png':...  \n",
       "2  {'https://www.svf.gov.lk/images/homeicon.png':...  \n",
       "3  {'https://www.svf.gov.lk/images/homeicon.png':...  \n",
       "4  {'https://www.svf.gov.lk/images/homeicon.png':...  "
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas dataframe for better visualisation\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions for Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['menu', 'home', 'about', 'contributions', 'services', 'downloads', 'gallery', 'news', '&', 'events', \n",
    "          'donate', 'vacancy', 'faqs', 'contact', 'us', 'sitemap', 'shrama', 'vasana', 'fund'] # words from navigation bar or frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(text_list):\n",
    "    \"\"\"Remove duplicates and words in remove list from the text list.\n",
    "\n",
    "    Args:\n",
    "        text_list: list of phrases.\n",
    "\n",
    "    Returns:\n",
    "        A list of words that are unique and not in the remove list.\n",
    "    \"\"\"\n",
    "    removed = []\n",
    "    for phrase in text_list: \n",
    "        words = phrase.split()\n",
    "        for word in words: \n",
    "            if word not in removed:\n",
    "                if word.lower() not in remove:\n",
    "                    removed.append(word)\n",
    "    return removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(text_list): \n",
    "    \"\"\"Cleans text data but does not include stemming and lemmatization, simply cleans the text data.\n",
    "      Trailing spaces are removed, words are converted to lower case. \n",
    "      Special characters and punctuations are removed from words.\n",
    "      Removal of stop words.\n",
    "    \n",
    "    Args:\n",
    "        text_list: list of phrases.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned list of words.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for word in text_list:\n",
    "        word = word.strip().lower() # remove spaces and convert to lowercase \n",
    "        word = re.sub(r'[^A-Za-z0-9\\s]', '', word) # remove special characters and punctuations from text (this also removes Sinhala language)\n",
    "        word = re.sub(r'\\n+', ' ', word) # replace \\n with space\n",
    "        if word and word not in stop_words: # remove stop words and empty strings\n",
    "            processed.append(word) \n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.ibm.com/topics/stemming-lemmatization#:~:text=The%20practical%20distinction%20between%20stemming,be%20found%20in%20the%20dictionary.\n",
    "\n",
    "def transform_texts(text_list): \n",
    "    \"\"\"Cleans text data including stemming and lemmatising.\n",
    "      Trailing spaces are removed, words are converted to lower case. \n",
    "      Special characters and punctuations are removed from words.\n",
    "      Removal of stop words.\n",
    "    \n",
    "    Args:\n",
    "        text_list: list of phrases.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned list of words that are tokenised. \n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for phrase in text_list:\n",
    "        phrase = phrase.strip().lower() # remove spaces and convert to lowercase \n",
    "        phrase = re.sub(r'[^A-Za-z0-9\\s]', '', phrase) # remove special characters and punctuations from text\n",
    "        phrase = re.sub(r'\\n+', ' ', phrase) # replace \\n with space\n",
    "        tokens = word_tokenize(phrase) # tokenise text\n",
    "        tokens = [word for word in tokens if word not in stop_words] # remove stop words\n",
    "        \n",
    "        #stemming\n",
    "        stemming = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        #lemmatization\n",
    "        lem = [lemmatizer.lemmatize(token) for token in stemming]\n",
    "\n",
    "        processed.append(lem) \n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_text_list(text_list): \n",
    "    \"\"\"Helper function for topic_modelling(tokenised_text_list), tokenises text to feed into lda_model.\n",
    "    \n",
    "    Args:\n",
    "        text_list: list of phrases.\n",
    "\n",
    "    Returns:\n",
    "        A cleaned list of words that are tokenised. \n",
    "    \"\"\"\n",
    "    processed = []\n",
    "\n",
    "    for phrase in text_list:\n",
    "        phrase = phrase.strip().lower() # remove spaces and convert to lowercase \n",
    "        phrase = re.sub(r'[^A-Za-z0-9\\s]', '', phrase) # remove special characters and punctuations from text\n",
    "        phrase = re.sub(r'\\n+', ' ', phrase) # replace \\n with space\n",
    "        tokens = word_tokenize(phrase) # tokenise text\n",
    "        \n",
    "        tokens = [word for word in tokens if word not in stop_words] # remove stop words\n",
    "        processed.append(tokens) \n",
    "\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf\n",
    "\n",
    "def topic_modelling(text_list):\n",
    "    \"\"\"Using the lda_model, generate a list of topics with weights from a given text list.\n",
    "    \n",
    "    Args:\n",
    "        text_list: list of phrases.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples consisting topics and weights. \n",
    "    \"\"\"\n",
    "    topic_weights = [] # store topics and weights in a list [(topics, weights)]\n",
    "    texts = tokenise_text_list(text_list)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]  \n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=20)\n",
    "    topics = lda_model.print_topics(num_words=1)\n",
    "\n",
    "    for index, topic in topics:\n",
    "        weight_str, topic_str = topic.split(\"*\")\n",
    "        topic = topic_str.replace('\"', '')\n",
    "        weight = float(weight_str)\n",
    "        topic_weights.append((topic, weight))\n",
    "    return topic_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(phrases_list): \n",
    "    \"\"\"Generates a word count from a given list of words.\n",
    "    \n",
    "    Args:\n",
    "        phrases_list: list of phrases/words.\n",
    "\n",
    "    Returns:\n",
    "        An integer representing the word count.\n",
    "    \"\"\"\n",
    "    word_count = 0 \n",
    "    for phrase in phrases_list: \n",
    "        word_count += len(phrase.split())\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Possible text entities \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text):\n",
    "    \"\"\"Generates a possible list of questions from a given text.\n",
    "    \n",
    "    Args:\n",
    "        text: Any text string.\n",
    "\n",
    "    Returns:\n",
    "        A list of questions.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm') # load spacy language model\n",
    "    text = nlp(text) # process text\n",
    "    questions = []\n",
    "    \n",
    "    for entity in text.ents: # look for potential nouns/subjects, generate possible questions with who/where/what\n",
    "        if not any(char.isdigit() for char in entity.text):\n",
    "            if entity.label_ == 'GPE':\n",
    "                question = f\"Where is {entity.text}?\"\n",
    "                questions.append(question) \n",
    "            # elif entity.label_ == 'DATE':\n",
    "            #     question = f\"When did {entity.text} happen?\"\n",
    "                # questions.append(question) \n",
    "            elif entity.label == 'PERSON': \n",
    "                question = f\"Who is {entity.text}?\"\n",
    "                questions.append(question) \n",
    "            else: \n",
    "                question = f\"What is {entity.text}?\"\n",
    "                questions.append(question) \n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/googletrans/\n",
    "\n",
    "def translate_to_english(text_list):\n",
    "    \"\"\"Translates texts in a text_list from any langugage to english.\n",
    "    \n",
    "    Args:\n",
    "        text_list: A list of texts.\n",
    "\n",
    "    Returns:\n",
    "        A list of translated texts.\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    translator = Translator()\n",
    "    for text in text_list:\n",
    "        try:\n",
    "            if text:\n",
    "                translated = translator.translate(text, dest='en')\n",
    "                lst.append(translated.text)\n",
    "            else:\n",
    "                lst.append(\"\")\n",
    "        except Exception as e:\n",
    "            lst.append(\"\") \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(text_list):\n",
    "    \"\"\"Converts a text list to a tfidf matrix\n",
    "    \n",
    "    Args:\n",
    "        text_list: A list of texts.\n",
    "\n",
    "    Returns:\n",
    "        A sparse tfidf matrix\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer() # general tfidf vectorizer that converts a list of processed texts to a tfidf matrix\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_list)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Scraped Data (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [item['title'] for item in data]\n",
    "texts = [item['texts'] for item in data]\n",
    "pdf_extracted = [item['pdf_extracted'] for item in data]\n",
    "image_extracted = [item['image_extracted'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shrama vasana fund - home', 'shrama vasana fund - overview', 'shrama vasana fund - contributions', 'shrama vasana fund - services', 'shrama vasana fund - downloads', 'shrama vasana fund - image gallery', 'shrama vasana fund - video gallery', 'shrama vasana fund - news & events', 'shrama vasana fund - donate us', 'shrama vasana fund - vacancy', 'shrama vasana fund - faqs', 'shrama vasana fund - inquiry', 'shrama vasana fund - contact details', 'shrama vasana fund - sitemap']\n"
     ]
    }
   ],
   "source": [
    "# convert titles to lower case\n",
    "clean_titles = [t.lower() for t in titles]\n",
    "print(clean_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_texts = []\n",
    "for text_list in texts: \n",
    "    text = translate_to_english(text_list) # translate texts to english\n",
    "    text = remove_duplicates(text) # remove duplicates, words from remove list\n",
    "    text = process_texts(text) # convert to lower case, removal of special characters, stop words and punctuations \n",
    "    clean_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word count for lists of texts\n",
    "word_count = []\n",
    "\n",
    "for text_list in clean_texts: \n",
    "    word_count.append(count_words(text_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_texts = []  # extract only texts from image_extracted\n",
    "for i in range(len(image_extracted)):\n",
    "    text = []\n",
    "    for url, image_text in image_extracted[i].items(): \n",
    "        if image_text: \n",
    "            text.append(image_text)\n",
    "    image_texts.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_image_texts = []\n",
    "\n",
    "for text_list in image_texts: \n",
    "    text = translate_to_english(text_list) # translate text to english\n",
    "    text = remove_duplicates(text) # remove duplicates, remove words from remove list\n",
    "    text = process_texts(text) # convert to lower case, remove stop words, special characters and punctuations \n",
    "    clean_image_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_texts = [] # extract only texts from pdf_extracted\n",
    "for i in range(len(pdf_extracted)):\n",
    "    text = []\n",
    "    for url, pdf_text in pdf_extracted[i].items(): \n",
    "        text.append(pdf_text)\n",
    "\n",
    "    pdf_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pdf_texts = [] \n",
    "for text_list in pdf_texts: \n",
    "    text = translate_to_english(text_list) # translate text to english\n",
    "    text = remove_duplicates(text) # remove duplicates, remove words from remove list\n",
    "    text = transform_texts(text) # convert to lower case, remove stop words, special characters and punctuations, stem and lemmatize words\n",
    "    clean_pdf_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate topics for lists of texts\n",
    "\n",
    "topic_modelling_texts = []\n",
    "\n",
    "for text_list in clean_texts: \n",
    "    topics = topic_modelling(text_list)\n",
    "    topic_modelling_texts.append(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate questions for lists of texts\n",
    "\n",
    "question_texts = []\n",
    "\n",
    "for text_list in clean_texts: \n",
    "    questions = generate_questions(\" \".join(text_list))\n",
    "    question_texts.append(questions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts list of text to tfidf matrix\n",
    "\n",
    "tfidf_texts = []\n",
    "\n",
    "for text_list in clean_texts: \n",
    "    tfidf = tfidf_matrix(text_list)\n",
    "    tfidf_texts.append(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {}\n",
    "for i in range(len(titles)): \n",
    "    processed_data[clean_titles[i]] = {\n",
    "        'texts': clean_texts[i], \n",
    "        'texts_word_count': word_count[i],\n",
    "        'texts_topics': topic_modelling_texts[i],\n",
    "        'texts_questions': question_texts[i],   \n",
    "        # 'texts_tfidf': tfidf_texts[i], \n",
    "        'pdf': clean_pdf_texts[i], \n",
    "        'image': clean_image_texts[i]  \n",
    "    }\n",
    "\n",
    "# texts still contain random integers (not sure if it should be removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processed_data\n",
    "| **label**                               | method of processing                                                                                                           |\n",
    "|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **titles**                               | titles of webpage converted to lower case |\n",
    "| **texts**                               | texts of webpage that has been cleaned (translated texts are converted to lower case, stop words, special characters and words from remove list are removed) |\n",
    "| **texts_word_count**                    | word count of texts (excluding image and pdf texts) on the webpage                                                                                                                  |\n",
    "| **texts_topics**                        | possible topics obtained topic modelling of texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **texts_questions**                     | possible questions obtained from texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **texts_tfidf**                         | tfidf matrix obtained from list of texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **pdf**                                 | translated pdf texts that has been cleaned (converted to lower case, stop words, special characters and words from remove list are removed, words are also tokenised by stemming and lemmatization)                                                                                                                  |\n",
    "| **image**                               | extracted image texts of webpage that has been cleaned (translated texts are converted to lower case, stop words, special characters and words from remove list are removed)                                                                                                                 |\n",
    "\n",
    "\n",
    "Note: Stemming and lemmatization was not conducted on webpage's text data as scraped webpage data is noisy, unstructured and fragmented. Stemming does not consider the context and reduces words to its base form. Although lemmatisation considers the grammar, it struggles to find the correct lemma when texts are fragmented.\n",
    "\n",
    "- PDF texts does not make sense when translated to english. Translation also seem to be inaccurate. \n",
    "- To consider storing image texts/pdf texts/texts without translating to english\n",
    "- To consider advanced contextual NLP techniques: Named entity recognition (NER) (already used in topic modelling), part-of-speech tagging, or word embeddings (e.g., Word2Vec, BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Scraped Data (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [item['title'] for item in data]\n",
    "texts = [item['texts'] for item in data]\n",
    "pdf_extracted = [item['pdf_extracted'] for item in data]\n",
    "image_extracted = [item['image_extracted'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert titles to lower case\n",
    "\n",
    "clean_titles2 = [t.lower() for t in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert texts to lower case, remove duplicates, stop words and words that are in remove list \n",
    "\n",
    "temp = [remove_duplicates(text_list) for text_list in texts]\n",
    "clean_texts2 = []\n",
    "for text_list in temp: \n",
    "    lst = []\n",
    "    for text in text_list: \n",
    "        text = text.lower()\n",
    "        lst.append(text)\n",
    "    clean_texts2.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word count for lists of texts\n",
    "\n",
    "word_count2 = []\n",
    "for text_list in clean_texts2: \n",
    "    word_count2.append(count_words(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate possible topics for lists of texts\n",
    "\n",
    "topic_modelling_texts2 = []\n",
    "\n",
    "for text_list in clean_texts2: \n",
    "    topics = topic_modelling(text_list)\n",
    "    topic_modelling_texts2.append(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate possible questions for lists of texts\n",
    "question_texts2 = []\n",
    "\n",
    "for text_list in clean_texts2: \n",
    "    questions = generate_questions(\" \".join(text_list))\n",
    "    question_texts2.append(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text lists to tfidf matrix\n",
    "tfidf_texts2 = []\n",
    "\n",
    "for text_list in clean_texts2: \n",
    "    tfidf = tfidf_matrix(text_list)\n",
    "    tfidf_texts2.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert pdf texts to lower case, remove duplicates, stop words and words that are in remove list \n",
    "\n",
    "temp = [remove_duplicates(text_list) for text_list in pdf_texts]\n",
    "clean_pdf_texts2= []\n",
    "for text_list in temp: \n",
    "    lst = []\n",
    "    for text in text_list: \n",
    "        text = text.lower()\n",
    "        lst.append(text)\n",
    "    clean_pdf_texts2.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    " # convert image texts to lower case, remove duplicates, stop words and words that are in remove list \n",
    "\n",
    "temp = [remove_duplicates(text_list) for text_list in image_texts]\n",
    "clean_image_texts2= []\n",
    "for text_list in temp: \n",
    "    lst = []\n",
    "    for text in text_list: \n",
    "        text = text.lower()\n",
    "        lst.append(text)\n",
    "    clean_image_texts2.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data2 = {}\n",
    "for i in range(len(titles)): \n",
    "    processed_data2[clean_titles2[i]] = {\n",
    "        'texts': clean_texts2[i], \n",
    "        'texts_word_count': word_count2[i],\n",
    "        'texts_topics': topic_modelling_texts2[i],\n",
    "        'texts_questions': question_texts2[i],   \n",
    "        #'texts_tfidf': tfidf_texts2[i], \n",
    "        'pdf': clean_pdf_texts2[i], \n",
    "        'image': clean_image_texts2[i]  \n",
    "    } \n",
    "\n",
    "# texts still contain random integers (not sure if it should be removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processed_data2\n",
    "| **label**                               | method of processing                                                                                                           |\n",
    "|-------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **titles**                               | titles of webpage converted to lower case |\n",
    "| **texts**                               | texts of webpage that has been cleaned (original texts are converted to lower case, stop words and words from remove list are removed) |\n",
    "| **texts_word_count**                    | word count of texts (excluding image and pdf texts) on the webpage                                                                                                                  |\n",
    "| **texts_topics**                        | possible topics obtained topic modelling of texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **texts_questions**                     | possible questions obtained from texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **texts_tfidf**                         | tfidf matrix obtained from list of texts (excluding image and pdf texts) from the webpage                                                                                                                  |\n",
    "| **pdf**                                 | pdf texts that has been cleaned (orginal texts are converted to lower case, stop words and words from remove list are removed)                                                                                                                  |\n",
    "| **image**                               | image texts of webpage that has been cleaned (original texts are converted to lower case, stop words and words from remove list are removed)                                                                                                                 |\n",
    "\n",
    "\n",
    "Note: Key differences between processed_data and processed_data2\n",
    "- Translation from Sinhala language to English is NOT conducted\n",
    "- Words that contain special characters are NOT removed (Sinhala language remains)\n",
    "- Stemming and lemmatization NOT conducted (it should only be carried out when special characters are removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data to MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mongo_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmongo_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserver_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServerApi\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmongo_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_database, insert_many_documents, find_all_documents\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mongo_utils'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from mongo_utils import get_database, insert_many_documents, find_all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = os.getenv(\"MONGO_DB_PASSWORD\")\n",
    "username = os.getenv(\"MONGO_DB_USERNAME\")\n",
    "\n",
    "uri = f\"mongodb+srv://{username}:{password}@bt4103.cnngw.mongodb.net/?retryWrites=true&w=majority&appName=BT4103&tlsCAFile=isrgrootx1.pem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 66f53ed5c5bd3090516d5d98\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "db = client[\"shrama_vasana_fund_uat\"]\n",
    "collection = db[\"processed_data\"]\n",
    "\n",
    "# Insert the data into the collection\n",
    "result = collection.insert_one(processed_data)\n",
    "print(f\"Inserted document with ID: {result.inserted_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted document with ID: 66f53ed6c5bd3090516d5d99\n"
     ]
    }
   ],
   "source": [
    "collection2 = db[\"processed_data2\"]\n",
    "\n",
    "# Insert the data into the collection\n",
    "result = collection2.insert_one(processed_data2)\n",
    "print(f\"Inserted document with ID: {result.inserted_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use find_all_documents to retrieve documents from MongoDB \n",
    "# print(find_all_documents(collection))\n",
    "# print(find_all_documents(collection2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Removed tfidf matrices from the processed datasets as MongoDB does not support sparse matrix, to think of another way to store tfidf matrices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
