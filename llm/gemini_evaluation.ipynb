{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key='AIzaSyAQH-5fbJEpH2K_D0FafxzIsv0BPp4NJaM')\n",
    "\n",
    "with open('data_selenium.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "cleaned_data = {}\n",
    "\n",
    "for item in data:\n",
    "    cleaned_title = ''\n",
    "    for (key, value) in item.items():\n",
    "        if key == 'title':\n",
    "            cleaned_title = value\n",
    "            break\n",
    "    item['texts'] = item['texts'] + item['image_extracted']\n",
    "    item['texts'] = item['texts'].replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \").replace(\"/=\", \"\")\n",
    "    cleaned_data[cleaned_title] = item\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for page_title, data in cleaned_data.items():\n",
    "    training_data.append({\n",
    "        'title': page_title,\n",
    "        'texts': data['texts'],\n",
    "    })\n",
    "\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "model = 'models/text-embedding-004'\n",
    "\n",
    "def embed_fn(title, text):\n",
    "  return genai.embed_content(model=model,\n",
    "                             content=text,\n",
    "                             task_type=\"retrieval_document\",\n",
    "                             title=title)[\"embedding\"]\n",
    "\n",
    "training_df['Embeddings'] = training_df.apply(lambda row: embed_fn(row['title'], row['texts']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = genai.GenerativeModel('gemini-1.5-pro-002')\n",
    "\n",
    "# Function to find the best passage based on embeddings\n",
    "def find_best_passage(query, dataframe):\n",
    "  query_embedding = genai.embed_content(model=model,\n",
    "                                        content=query,\n",
    "                                        task_type=\"retrieval_query\")\n",
    "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding[\"embedding\"])\n",
    "  idx = np.argmax(dot_products)\n",
    "  return dataframe.iloc[idx]['texts']\n",
    "\n",
    "# Function to create prompt based on query and relevant passage\n",
    "def make_prompt(query, relevant_passage, convo_history):\n",
    "    escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "\n",
    "    prompt = textwrap.dedent(f\"\"\"\\\n",
    "    You are a helpful and informative bot that answers questions using text from the reference passage and conversation history included below. \\\n",
    "    You may also need to refer to contextual clues from the conversation history provided when crafting your answer. \\\n",
    "    Please answer to the best of your ability. Do not mention the context to your audience, just answer their questions.\\\n",
    "    Be sure to respond in complete sentences and break them into succinct paragraphs and bulletpoints for readability.\\\n",
    "    Please be comprehensive and include all relevant background information. \\\n",
    "    If the passages and previous conversation history are irrelevant to the answer, you may ignore it. \\\n",
    "                             \n",
    "    PREVIOUS CONVERSATION: '{convo_history}'                         \n",
    "    QUESTION: '{query}'\n",
    "    PASSAGE: '{relevant_passage}'\n",
    "    ANSWER:\n",
    "    \"\"\").format(query=query, relevant_passage=escaped,convo_history=convo_history)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def get_response(query, convo_history):\n",
    "    passage = find_best_passage(query, training_df)\n",
    "    prompt = make_prompt(query, passage, convo_history)\n",
    "    answer = gemini_model.generate_content(prompt)\n",
    "    return answer.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cases.json', 'r') as file:\n",
    "    test_cases = json.load(file)\n",
    "    \n",
    "histories = []\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "for (key, value) in test_cases.items():\n",
    "    test_case = value\n",
    "    for (role, text) in value.items():\n",
    "        if role[0] == 'Q':\n",
    "            prompts.append((text + ','))\n",
    "        else:\n",
    "            responses.append((text+ ','))\n",
    "        history = ''\n",
    "        for (role1, text1) in test_case.items():\n",
    "            if text1 != text:\n",
    "                history += text1 + ';'\n",
    "            else:\n",
    "                break\n",
    "        histories.append((history + ','))\n",
    "\n",
    "del histories[1::2]\n",
    "\n",
    "baseline_performance = pd.DataFrame({\n",
    "    \"history\": histories,\n",
    "    \"prompt\": prompts,\n",
    "    \"response\": responses,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_responses = []\n",
    "gemini_histories = []\n",
    "hist = ''\n",
    "\n",
    "for (index, row) in baseline_performance.iterrows():\n",
    "    prompt = row.iloc[1]\n",
    "    if row.iloc[0] == ',':\n",
    "        hist = ''\n",
    "    resp = get_response(prompt, hist)\n",
    "    gemini_responses.append(resp)\n",
    "    hist += prompt + '; ' + resp + '; '\n",
    "    gemini_histories.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import (\n",
    "    EvalTask,\n",
    "    PointwiseMetric,\n",
    "    PointwiseMetricPromptTemplate,\n",
    "    MetricPromptTemplateExamples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pointwise metric with two custom criteria\n",
    "custom_text_quality = PointwiseMetric(\n",
    "    metric=\"custom_text_quality\",\n",
    "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "        criteria={\n",
    "          \"fluency\": \"Sentences flow smoothly and are easy to read, avoiding awkward phrasing or run-on sentences. Ideas and sentences connect logically, using transitions effectively where needed.\",\n",
    "          \"coherence\": \"The sentences should demonstrate a logical flow, where ideas progress smoothly with clear transitions, and maintain relevance to the main point. Effective organization is essential, with a clear structure, signaling, and topic sentences to guide the reader. Additionally, the writing should exhibit strong cohesion, using word choices, sentence structures, pronouns, and figurative language to reinforce connections between ideas and create a unified piece.\",\n",
    "        },\n",
    "        rating_rubric={\n",
    "          \"1\": \"The response performs well on both criteria.\",\n",
    "          \"0\": \"The response is somewhat aligned with both criteria\",\n",
    "          \"-1\": \"The response falls short on both criteria\",\n",
    "        },\n",
    "        input_variables=[\"prompt\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the serialized metric prompt template\n",
    "# print(custom_text_quality.metric_prompt_template)\n",
    "\n",
    "pointwise_eval_dataset = pd.DataFrame({\n",
    "    \"prompt\": prompts,\n",
    "    \"response\" : gemini_responses,\n",
    "})\n",
    "\n",
    "sample_eval_dataset = pointwise_eval_dataset.sample(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 1 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:31<00:00, 571.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 errors encountered during evaluation. Continue to compute summary metrics for the rest of the dataset.\n",
      "Error encountered for metric custom_text_quality at dataset index 22: Error: Timeout of 600.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 403,\\n    \"message\": \"Permission \\'iam.serviceAccounts.getAccessToken\\' denied on resource (or it may not exist).\",\\n    \"status\": \"PERMISSION_DENIED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n        \"reason\": \"IAM_PERMISSION_DENIED\",\\n        \"domain\": \"iam.googleapis.com\",\\n        \"metadata\": {\\n          \"permission\": \"iam.serviceAccounts.getAccessToken\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n')\n",
      "Evaluation Took:571.6326454000009 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>custom_text_quality/explanation</th>\n",
       "      <th>custom_text_quality/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>my work place closed down without prior notice,</td>\n",
       "      <td>The Shrama Vasana Fund may be able to provide ...</td>\n",
       "      <td>Error</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            prompt  \\\n",
       "0  my work place closed down without prior notice,   \n",
       "\n",
       "                                            response  \\\n",
       "0  The Shrama Vasana Fund may be able to provide ...   \n",
       "\n",
       "  custom_text_quality/explanation custom_text_quality/score  \n",
       "0                           Error                      None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evaluation using the custom_text_quality metric\n",
    "eval_task = EvalTask(\n",
    "    dataset=sample_eval_dataset,\n",
    "    metrics=[custom_text_quality],\n",
    ")\n",
    "eval_result = eval_task.evaluate()\n",
    "eval_result.metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pointwise multi-turn chat quality metric\n",
    "pointwise_chat_quality_metric_prompt = \"\"\"Evaluate the AI's contribution to a meaningful conversation, considering coherence, fluency, groundedness, and conciseness.\n",
    " Review the chat history for context. Rate the response on a 1-5 scale, with explanations for each criterion and its overall impact.\n",
    "\n",
    "# Conversation History\n",
    "{history}\n",
    "\n",
    "# Current User Prompt\n",
    "{prompt}\n",
    "\n",
    "# AI-generated Response\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "freeform_multi_turn_chat_quality_metric = PointwiseMetric(\n",
    "    metric=\"multi_turn_chat_quality_metric\",\n",
    "    metric_prompt_template=pointwise_chat_quality_metric_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 1 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [09:25<00:00, 565.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 errors encountered during evaluation. Continue to compute summary metrics for the rest of the dataset.\n",
      "Error encountered for metric multi_turn_chat_quality_metric at dataset index 9: Error: Timeout of 600.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: ('Unable to acquire impersonated credentials', '{\\n  \"error\": {\\n    \"code\": 403,\\n    \"message\": \"Permission \\'iam.serviceAccounts.getAccessToken\\' denied on resource (or it may not exist).\",\\n    \"status\": \"PERMISSION_DENIED\",\\n    \"details\": [\\n      {\\n        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\\n        \"reason\": \"IAM_PERMISSION_DENIED\",\\n        \"domain\": \"iam.googleapis.com\",\\n        \"metadata\": {\\n          \"permission\": \"iam.serviceAccounts.getAccessToken\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n')\n",
      "Evaluation Took:565.9153282999978 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pairwise_eval_data = pd.DataFrame({\n",
    "    \"history\": gemini_histories,\n",
    "    \"prompt\": prompt,\n",
    "    \"response\": gemini_responses\n",
    "})\n",
    "\n",
    "pairwise_eval_data = pairwise_eval_data.sample(1)\n",
    "\n",
    "# Run evaluation using the freeform_multi_turn_chat_quality_metric metric\n",
    "eval_task = EvalTask(\n",
    "    dataset=pairwise_eval_data,\n",
    "    metrics=[freeform_multi_turn_chat_quality_metric],\n",
    ")\n",
    "eval_result = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine computation-based metrics \"ROUGE\" and \"BLEU\" with model-based metrics\n",
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=[\"rouge_l_sum\", \"bleu\", custom_text_quality],\n",
    ")\n",
    "eval_result = eval_task.evaluate(\n",
    "    model=gemini_model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
